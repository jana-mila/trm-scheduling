{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fc4f1-2cf3-4adf-af82-1371fe48731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(__file__).resolve().parent / \"TinyRecursiveModels\"))\n",
    "from TinyRecursiveModels.models.recursive_reasoning.trm import *\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, Optional\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ec6e8-d1bc-432d-921e-2ce5652859ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "# --- Hyperparameters ---\n",
    "TRAIN_PROBLEMS_DIR = 'data/train/problems'\n",
    "TRAIN_SOLUTIONS_DIR = 'data/train/solutions'\n",
    "VAL_PROBLEMS_DIR = 'data/val/problems'\n",
    "VAL_SOLUTIONS_DIR = 'data/val/solutions'\n",
    "CHKPT_DIR = 'checkpoints'\n",
    "CHKPT_DIR = Path(CHKPT_DIR)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "HALT_MAX_STEPS = 16 # This is N_sup, the number of \"Deep Supervision\" steps\n",
    "ACT_LOSS_WEIGHT = 0.01  # Weight for the ACT (halting) loss\n",
    "\n",
    "# --- Model & Data Shape Config ---\n",
    "# We must pad all problems to a fixed size.\n",
    "MAX_J = 20\n",
    "MAX_T = 20\n",
    "MAX_SEQ_LEN = MAX_J * MAX_T # This is the 'seq_len' for the model\n",
    "HIDDEN_SIZE = 256 # Model's internal dimension\n",
    "PUZZLE_EMB_DIM = 64 # Dimension for the \"task ID\" embedding\n",
    "\n",
    "model_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=MAX_SEQ_LEN,\n",
    "    puzzle_emb_ndim=PUZZLE_EMB_DIM,\n",
    "    num_puzzle_identifiers=1, # We only have one task: \"JSP\"\n",
    "    vocab_size=0,             # We are not using a token vocab (CRITICAL)\n",
    "    puzzle_emb_len=16,\n",
    "    \n",
    "    # Recursive reasoning config\n",
    "    H_cycles=3,               # T=3 in the paper, a good default\n",
    "    L_cycles=6,               # n=6 in the paper, a good default\n",
    "    L_layers=2,               # \"tiny\" 2-layer model from paper\n",
    "    H_layers=0,               # This is ignored since the TRM simplified the hierarchy\n",
    "\n",
    "    # Transformer config\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    expansion=2.0,            # Standard for SwiGLU\n",
    "    num_heads=4,              # Must be a divisor of HIDDEN_SIZE\n",
    "    pos_encodings=\"rope\",\n",
    "\n",
    "    # Halting Q-learning config\n",
    "    halt_max_steps=HALT_MAX_STEPS,\n",
    "    halt_exploration_prob=0.0, # Not needed for simple training\n",
    "    no_ACT_continue=True,      # Use the simplified ACT loss from paper\n",
    "\n",
    "    forward_dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660be15-c36a-4a9d-a177-bde0a7bda424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# DATASET AND DATALOADER\n",
    "# -----------------------------\n",
    "\n",
    "class CustomJobShopDataset(Dataset):\n",
    "    def __init__(self, problems_dir, solutions_dir, transform=None):\n",
    "        self.problems_dir = Path(problems_dir)\n",
    "        self.solutions_dir = Path(solutions_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.problems_files = sorted(self.problems_dir.glob('*.pt'))\n",
    "        self.solutions_files = sorted(self.solutions_dir.glob('*.pt'))\n",
    "\n",
    "        self.num_problems = len(self.problems_files)\n",
    "        assert self.num_problems == len(self.solutions_files), \"Problem and solution file count mismatch\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_problems\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = torch.load(self.problems_files[idx])\n",
    "        solution = torch.load(self.solutions_files[idx])\n",
    "        \n",
    "        # flatten from (J,T,C) to (L,C)\n",
    "        problem = problem.view(-1, 2)\n",
    "        solution = solution.view(-1, 1)\n",
    "        # we need to provide puzzle_identifier, 0 is the ID for our JSP task\n",
    "        puzzle_identifier = torch.tensor(0, dtype=torch.int32)\n",
    "        \n",
    "        return {\n",
    "            \"inputs\": problem,\n",
    "            \"labels\": solution,\n",
    "            \"puzzle_identifiers\": puzzle_identifier\n",
    "        }\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    max_len = max(item['inputs'].shape[0] for item in batch)\n",
    "    if max_len > MAX_SEQ_LEN:\n",
    "        max_len = MAX_SEQ_LEN\n",
    "\n",
    "    # Pad inputs (the (M, D) puzzle)\n",
    "    padded_problems = torch.zeros(len(batch), MAX_SEQ_LEN, 2, dtype=torch.float32)\n",
    "    # Pad labels (the (S) solution)\n",
    "    padded_solutions = torch.full((len(batch), MAX_SEQ_LEN, 1), -1.0, dtype=torch.float32)\n",
    "    # Mask to identify real data vs. padding\n",
    "    mask = torch.zeros(len(batch), MAX_SEQ_LEN, 1, dtype=torch.bool)\n",
    "    \n",
    "    identifiers = []\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        seq_len = item['inputs'].shape[0]\n",
    "        \n",
    "        # Truncate if longer than max len\n",
    "        if seq_len > MAX_SEQ_LEN:\n",
    "            seq_len = MAX_SEQ_LEN\n",
    "            \n",
    "        padded_problems[i, :seq_len] = item['inputs'][:seq_len]\n",
    "        padded_solutions[i, :seq_len] = item['labels'][:seq_len]\n",
    "        mask[i, :seq_len] = True\n",
    "        identifiers.append(item['puzzle_identifiers'])\n",
    "\n",
    "    return {\n",
    "        'inputs': padded_problems,\n",
    "        'labels': padded_solutions,\n",
    "        'mask': mask,\n",
    "        'puzzle_identifiers': torch.stack(identifiers)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804121b-d12c-413e-9f6f-0427541365aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "There are 1000 training job shop problem samples\n",
      "There are 200 validation job shop problem samples\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# INITIALIZE DATA LOADERS\n",
    "# -----------------------------\n",
    "print(\"Starting training...\")\n",
    "train_dataset = CustomJobShopDataset(TRAIN_PROBLEMS_DIR, TRAIN_SOLUTIONS_DIR)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "print(f\"There are {len(train_dataset)} training job shop problem samples\")\n",
    "\n",
    "val_dataset = CustomJobShopDataset(VAL_PROBLEMS_DIR, VAL_SOLUTIONS_DIR)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "print(f\"There are {len(val_dataset)} validation job shop problem samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ddd44-f9eb-47c7-9e15-f82b260e27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model...\n",
      "Using device: cpu\n",
      "Model created with 1,312,002 parameters.\n",
      "Model is as follows: \n",
      "TinyRecursiveReasoningModel_ACTV1(\n",
      "  (inner): TinyRecursiveReasoningModel_ACTV1_Inner(\n",
      "    (input_proj): CastedLinear()\n",
      "    (lm_head): CastedLinear()\n",
      "    (q_head): CastedLinear()\n",
      "    (puzzle_emb): CastedSparseEmbedding()\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "    (L_level): TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TinyRecursiveReasoningModel_ACTV1Block(\n",
      "          (self_attn): Attention(\n",
      "            (qkv_proj): CastedLinear()\n",
      "            (o_proj): CastedLinear()\n",
      "          )\n",
      "          (mlp): SwiGLU(\n",
      "            (gate_up_proj): CastedLinear()\n",
      "            (down_proj): CastedLinear()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# MODEL, LOSS, OPTIMIZER\n",
    "# -----------------------------\n",
    "print(\"Setting up model...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TinyRecursiveReasoningModel_ACTV1(model_cfg)\n",
    "loss_fn = nn.MSELoss(reduction='none') # 'none' to apply mask manually\n",
    "act_loss_fn = nn.BCEWithLogitsLoss() # For the halting signal\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters.\")\n",
    "print(f\"Model is as follows: \\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fff45-8634-40a0-aa4c-646d90a155af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjana-mila\u001b[0m (\u001b[33mjana-mila-mila\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/janaosea/Projects/trm-scheduling/wandb/run-20251110_170312-40hjf7xm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jana-mila-mila/trm-scheduling/runs/40hjf7xm' target=\"_blank\">1110_1703</a></strong> to <a href='https://wandb.ai/jana-mila-mila/trm-scheduling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jana-mila-mila/trm-scheduling' target=\"_blank\">https://wandb.ai/jana-mila-mila/trm-scheduling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jana-mila-mila/trm-scheduling/runs/40hjf7xm' target=\"_blank\">https://wandb.ai/jana-mila-mila/trm-scheduling/runs/40hjf7xm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jana-mila-mila/trm-scheduling/runs/40hjf7xm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x164144980>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# WANDB\n",
    "# -----------------------------\n",
    "now = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "exp_name = f\"{now}\"\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"trm-scheduling\",\n",
    "    name=exp_name,\n",
    "    config={\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": LR,\n",
    "        **model_cfg\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915b827-41ba-4abd-8c4e-743d50a9b735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m total_loss_for_batch = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(HALT_MAX_STEPS):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     carry, outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     logits = outputs[\u001b[33m'\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m'\u001b[39m]             \n\u001b[32m     20\u001b[39m     labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/recursive_reasoning/trm.py:281\u001b[39m, in \u001b[36mTinyRecursiveReasoningModel_ACTV1.forward\u001b[39m\u001b[34m(self, carry, batch)\u001b[39m\n\u001b[32m    278\u001b[39m new_current_data = {k: torch.where(carry.halted.view((-\u001b[32m1\u001b[39m, ) + (\u001b[32m1\u001b[39m, ) * (batch[k].ndim - \u001b[32m1\u001b[39m)), batch[k], v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m carry.current_data.items()}\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Forward inner model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m new_inner_carry, logits, (q_halt_logits, q_continue_logits) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inner_carry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_current_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m outputs = {\n\u001b[32m    284\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m: logits,\n\u001b[32m    285\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mq_halt_logits\u001b[39m\u001b[33m\"\u001b[39m: q_halt_logits,\n\u001b[32m    286\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mq_continue_logits\u001b[39m\u001b[33m\"\u001b[39m: q_continue_logits\n\u001b[32m    287\u001b[39m }\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# Step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/recursive_reasoning/trm.py:232\u001b[39m, in \u001b[36mTinyRecursiveReasoningModel_ACTV1_Inner.forward\u001b[39m\u001b[34m(self, carry, batch)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _H_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.H_cycles-\u001b[32m1\u001b[39m):\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _L_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.L_cycles):\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m             z_L = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mL_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_L\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_H\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mseq_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m         z_H = \u001b[38;5;28mself\u001b[39m.L_level(z_H, z_L, **seq_info)\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# 1 with grad\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/recursive_reasoning/trm.py:114\u001b[39m, in \u001b[36mTinyRecursiveReasoningModel_ACTV1ReasoningModule.forward\u001b[39m\u001b[34m(self, hidden_states, input_injection, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m hidden_states = hidden_states + input_injection\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     hidden_states = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/recursive_reasoning/trm.py:102\u001b[39m, in \u001b[36mTinyRecursiveReasoningModel_ACTV1Block.forward\u001b[39m\u001b[34m(self, cos_sin, hidden_states)\u001b[39m\n\u001b[32m    100\u001b[39m     hidden_states = rms_norm(hidden_states + \u001b[38;5;28mself\u001b[39m.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=\u001b[38;5;28mself\u001b[39m.norm_eps)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m hidden_states = rms_norm(hidden_states + out, variance_epsilon=\u001b[38;5;28mself\u001b[39m.norm_eps)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/layers.py:161\u001b[39m, in \u001b[36mSwiGLU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    160\u001b[39m     gate, up = \u001b[38;5;28mself\u001b[39m.gate_up_proj(x).chunk(\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/TinyRecursiveModels/models/layers.py:60\u001b[39m, in \u001b[36mCastedLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x13ebd70e0>> (for post_run_cell), with arguments args (<ExecutionResult object at 16402be30, execution_count=7 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 16402b5c0, raw_cell=\"# %%\n",
      "# -----------------------------\n",
      "# TRAINING\n",
      "# ..\" transformed_cell=\"# %%\n",
      "# -----------------------------\n",
      "# TRAINING\n",
      "# ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X10sdW50aXRsZWQ%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:596\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    595\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface.py:826\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    825\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/trm-scheduling/venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/streams.py:386\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TRAINING\n",
    "# -----------------------------\n",
    "best_val_loss = 1e10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n ---Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    model.train()\n",
    "    train_epoch_loss = 0.0\n",
    "    train_batch = 0\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        carry = model.initial_carry(batch)\n",
    "        total_loss_for_batch = 0\n",
    "\n",
    "        for step in range(HALT_MAX_STEPS):\n",
    "            carry, outputs = model(carry=carry, batch=batch)\n",
    "            logits = outputs['logits']             \n",
    "            labels = batch['labels']\n",
    "            mask = batch['mask'] # (B, L, 1)\n",
    "\n",
    "            # --- 1. Calculate Main Task Loss (Regression) ---\n",
    "            step_loss_unmasked = loss_fn(logits, labels)\n",
    "            step_loss_masked = step_loss_unmasked * mask\n",
    "            \n",
    "            # Add a small epsilon to prevent division by zero if a batch has no real tokens\n",
    "            # (which shouldn't happen, but is good practice)\n",
    "            num_real_tokens = mask.sum() + 1e-8 \n",
    "            task_loss = step_loss_masked.sum() / num_real_tokens\n",
    "\n",
    "            # --- 2. Calculate ACT Loss (Halting) ---\n",
    "            # We want the model to halt when its answer is \"good enough\"\n",
    "            # For regression, \"good enough\" is complex. A simpler target\n",
    "            # is to just train it to halt on the *last step*.\n",
    "            \n",
    "            # Target is 0 (don't halt) for all steps except the last\n",
    "            is_last_step = (step == HALT_MAX_STEPS - 1)\n",
    "            halt_target = torch.full_like(outputs['q_halt_logits'], \n",
    "                                          float(is_last_step))\n",
    "            act_loss = act_loss_fn(outputs['q_halt_logits'], halt_target)\n",
    "\n",
    "            # --- 3. Combine Losses ---\n",
    "            total_step_loss = task_loss + (act_loss * ACT_LOSS_WEIGHT)\n",
    "            total_loss_for_batch += total_step_loss\n",
    "        \n",
    "         # Backpropagate the *sum* of losses from all 16 steps\n",
    "        optimizer.zero_grad()\n",
    "        total_loss_for_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_batch += 1        \n",
    "        train_epoch_loss += total_loss_for_batch.item()\n",
    "    \n",
    "    avg_train_epoch_loss = (train_epoch_loss / train_batch) / HALT_MAX_STEPS\n",
    "    print(f\"Epoch {epoch+1} finished. Average training loss: {avg_train_epoch_loss:.6f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATE\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_epoch_total_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, leave=False, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            carry = model.initial_carry(batch)\n",
    "            total_val_loss_for_batch = 0\n",
    "\n",
    "            for step in range(HALT_MAX_STEPS):\n",
    "                carry, outputs = model(carry=carry, batch=batch)\n",
    "                logits = outputs['logits']\n",
    "                labels = batch['labels']\n",
    "                mask = batch['mask']\n",
    "\n",
    "                step_loss_unmasked = loss_fn(logits, labels)\n",
    "                step_loss_masked = step_loss_unmasked * mask\n",
    "\n",
    "                num_real_tokens = mask.sum() + 1e-8 \n",
    "                task_loss = step_loss_masked.sum() / num_real_tokens\n",
    "\n",
    "                is_last_step = (step == HALT_MAX_STEPS - 1)\n",
    "                halt_target = torch.full_like(outputs['q_halt_logits'], float(is_last_step))\n",
    "                act_loss = act_loss_fn(outputs['q_halt_logits'], halt_target)\n",
    "\n",
    "                total_step_loss = task_loss + (act_loss * ACT_LOSS_WEIGHT)\n",
    "                total_loss_for_batch += total_step_loss\n",
    "\n",
    "            val_epoch_total_loss += total_loss_for_batch.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_val_loss = (val_epoch_total_loss / val_batches) / HALT_MAX_STEPS\n",
    "    print(f\"Epoch {epoch+1} Validation loss: {avg_val_loss: .6f}\") \n",
    "    wandb.log({'val_loss': avg_val_loss})\n",
    "\n",
    "    # -----------------------------\n",
    "    # CHECKPOINT (IF BEST)\n",
    "    # -----------------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        chckpt_name = f\"{exp_name}-epoch-{epoch}-valloss-{avg_val_loss:.4f}.pth\"\n",
    "        chkpt_file = CHKPT_DIR / chckpt_name\n",
    "        torch.save(model.state_dict(), chkpt_file)\n",
    "        best_val_loss = avg_val_loss\n",
    "        artifact = wandb.Artifact(name=chckpt_name, type='model')\n",
    "        artifact.add_file(chkpt_file)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"New best model saved at {chkpt_file}\")\n",
    "\n",
    "wand.finish()\n",
    "print('Training complete!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
